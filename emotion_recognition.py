# -*- coding: utf-8 -*-
"""emotion-recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q2AGahcrmn_tGiA9pLIYwKK1_00WJ7M5
"""

# load data set from Google drive
from google.colab import drive
drive.mount('/gdrive')

import pandas as pd
import numpy as np 
posts = pd.read_json("/gdrive/My Drive/nlp_train.json", orient='index')
posts = posts[['body', 'emotion']]

# one-to-one mapping
posts_bodies = []
posts_emotions = []
for column, post in posts.iterrows():
  for key, value in post["emotion"].items():
    if value == True:
      posts_bodies.append(post["body"])
      posts_emotions.append(key)
posts = pd.DataFrame({'label': posts_emotions, 'text': posts_bodies})
print(posts[0:2])

import pickle
from tensorflow.keras.preprocessing.text import Tokenizer

num_words = 10000
tokenizer = Tokenizer(num_words=num_words, lower=True)
tokenizer.fit_on_texts(posts_bodies)

file = open("/gdrive/My Drive/tokenizer.pickle", 'wb')
pickle.dump(tokenizer, file)

from sklearn.model_selection import train_test_split

train = pd.DataFrame(columns=['label', 'text'])
test = pd.DataFrame(columns=['label', 'text'])
validation = pd.DataFrame(columns=['label', 'text'])
for label in posts.label.unique():
    label_data = posts[posts.label == label]
    train_data, test_data = train_test_split(label_data, test_size=0.01)
    train_data, validation_data = train_test_split(train_data, test_size=0.2)
    train = pd.concat([train, train_data])
    validation = pd.concat([validation, validation_data])
    test = pd.concat([test, test_data])
print(len(train))
print(len(test))
print(len(validation))

from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM
from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D
from tensorflow.keras.layers import Bidirectional, Conv1D, Dense, concatenate
from tensorflow.keras.models import Model

input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)
num_classes = len(posts.label.unique())

input_layer = Input(shape=(100,))
output_layer = Embedding(input_dim=input_dim, output_dim=500, input_shape=(100,))(input_layer)
output_layer = SpatialDropout1D(0.2)(output_layer)
output_layer = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(output_layer)
output_layer = Conv1D(64, kernel_size=7, padding='valid', kernel_initializer='glorot_uniform')(output_layer)
avg_pool = GlobalAveragePooling1D()(output_layer)
max_pool = GlobalMaxPooling1D()(output_layer)
output_layer = concatenate([avg_pool, max_pool])
output_layer = Dense(num_classes, activation='softmax')(output_layer)

model = Model(input_layer, output_layer)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelBinarizer

train_sequences = [text.split() for text in train.text]
validation_sequences = [text.split() for text in validation.text]
list_tokenized_train = tokenizer.texts_to_sequences(train_sequences)
list_tokenized_validation = tokenizer.texts_to_sequences(validation_sequences)
x_train = pad_sequences(list_tokenized_train, maxlen=100)
x_validation = pad_sequences(list_tokenized_validation, maxlen=100)

encoder = LabelBinarizer()
encoder.fit(posts.label.unique())

file = open("/gdrive/My Drive/encoder.pickle", 'wb')
pickle.dump(encoder, file)

y_train = encoder.transform(train.label)
y_validation = encoder.transform(validation.label)

batch_size = 64
epochs = 10
model.fit(x_train, y=y_train, batch_size=64, epochs=10, validation_data=(x_validation, y_validation))

model.save_weights("/gdrive/My Drive/model_weights.h5")

from nlp.utils import preprocess
pred = []
for text in test.text:
  list_tokenized = tokenizer.texts_to_sequences(text.split())
  x_data = pad_sequences(list_tokenized, maxlen=50)
  y_pred = model.predict(x_data)
  emt = []
  for index, value in enumerate(np.sum(y_pred, axis=0) / len(y_pred)):
    emt.append(encoder.classes_[index] + ": " + str(value))
  pred.append(emt)

print(test.label[0:2])
print(pred[0:2])